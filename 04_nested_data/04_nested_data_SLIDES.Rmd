---
title: "Nested Data"
author: "Bernhard Piskernik"
date: "2022-11-17"
output: 
  ioslides_presentation:
        css: ../style.css
        incremental: true
        self_contained: true
---


```{r setup, include=FALSE}
library(tidyverse)
library(magrittr)
library(plotly)
library(kableExtra)
library(datapasta)
library(lme4)
library(lmeresampler)
options(warn=-1)
options("kableExtra.html.bsTable" = T)
theme_set(theme_minimal())
```

```{r data_load, include=FALSE}
# retrieve data of the article "Determinants of healthcare worker turnover in intensive care units: A micro-macro multilevel analysis" https://doi.org/10.1371/journal.pone.0251779

# Individual-level factors data: https://doi.org/10.1371/journal.pone.0251779.s010
# Intensive care unit-level factors data: https://doi.org/10.1371/journal.pone.0251779.s011

# add the folder /journal.pone.0251779 into /data and copy both files into it

df_individual <- read_csv2('../data/journal.pone.0251779/journal.pone.0251779.s010.csv')
df_unit <- read_csv2('../data/journal.pone.0251779/journal.pone.0251779.s011.csv')

df_ICU <- df_individual %>%
  left_join(df_unit) %>%
  mutate(
    CodeService = as_factor(CodeService),
    `type of ICU` = as_factor(`type of ICU`)
    )
```




## The Basic Problem {.build}

Multilevel models are developed for the analysis of hierarchically structured data. A hierarchy consists of lower level observations nested within higher level(s).

_Example_: 

* Level 1: measurement at one time
* Level 2: student
* Level 3: class
* Level 4: school
* Level 5: district
* ...

## The linear multilevel model {.build}

Special regression that is suitable for hierarchical data.

Difference to _traditional_ regression:<br>More than one error term (1+ per level)


Let _i_ be the index for Level 1 units ($i$ = 1,...,$n_j$) and _j_ be the index for Level 2 units ($j$ = 1,...,$J$) then the DV $Y_{ij}$ at Level 1 is explained by:

<center>
$Y_{ij}=\alpha_j+\beta_jX_{ij}+\epsilon_{ij}$

$\alpha_j= \mu+\gamma Z_j + u_j$<br>
$\beta_j=\theta+ \eta Z_j + v_j$
</center>
<br>
where $X_{ij}$ is a Level 1 variable, and $Z_j$ is a Level 2 variable

## Why we can't just use normal regression. {.flexbox .vcenter}

1. faithful to the data structure
2. individuals within a group are similar (_correlated_) and therefor include less information than independent individuals (effective _n_ is overestimated and SEs are too small)
3. effects on different levels don't necessarily need be the same

## similar cases share information

```{r, echo=FALSE, message=FALSE}
set.seed(4242)

rbind(
    MASS::mvrnorm(10, mu=c(1,2), Sigma=matrix(c(1,0.25,0.25,1),2,2)),
    MASS::mvrnorm(10, mu=c(4,3), Sigma=matrix(c(1,0.25,0.25,1),2,2)),
    MASS::mvrnorm(10, mu=c(7,10), Sigma=matrix(c(1,0.25,0.25,1),2,2)),
    MASS::mvrnorm(10, mu=c(10,10), Sigma=matrix(c(1,0.25,0.25,1),2,2))
  ) %>%
  as_tibble() %>%
  mutate(group = rep(c('A', 'B', 'C', 'D'), each=10)) %>%
  ggplot(aes(x=V1, y=V2)) +
    ggforce::geom_mark_ellipse(expand = 0,aes(fill=group))+
    geom_point(size=2, aes(color=group)) +
    geom_smooth(method=lm, se=F)
```

Would the regression line look much different with just one point per group?

## effects on different levels can differ

```{r, echo=FALSE, message=FALSE}
set.seed(4242)
# Create data where multilevel model gives different result 
# than OLS regression.  Hopefully see differences in both
# coefficients and SEs
# Simulation 1 - OLS coefficient is wrong direction
subject = c(rep(1,10),rep(2,10),rep(3,10),rep(4,10))
lambda0 = c(rep(10,10),rep(20,10),rep(30,10),rep(40,10))
lambda1 = rep(-0.5,40)
previj = c(1:10,4:13,7:16,10:19)
eij = rnorm(40,0,1)
yij = lambda0 + lambda1*previj + eij
simdata = data.frame(subject=subject,lambda0=lambda0,
  lambda1=lambda1,previj=previj,eij=eij,yij=yij)
#plot(yij~previj)
olsreg.sim = lm(yij~previj)
#summary(olsreg.sim)
#AIC(olsreg.sim); BIC(olsreg.sim)
mlm.sim = lmer(yij~previj + (1|subject), data=simdata)
#summary(mlm.sim)
# ggplot for first simulation
ints.sim = fixef(mlm.sim)[1] + ranef(mlm.sim)[[1]][1]
slopes.sim = rep(fixef(mlm.sim)[2],4)
subj.sim = c("Group 1", "Group 2", 
             "Group 3", "Group 4")
sim1.plot = data.frame(id=subj.sim,
  ints.sim=ints.sim[[1]],slopes.sim=slopes.sim)
sim1.plot2 = data.frame(model=c("MultiLevel","LinReg"),
  int2=c(fixef(mlm.sim)[1],
  summary(olsreg.sim)$coefficients[1,1]),
  slp2=c(fixef(mlm.sim)[2],
  summary(olsreg.sim)$coefficients[2,1]))

ggplot(data=simdata, aes(x=previj,y=yij, color=forcats::as_factor(subject))) +  
  geom_point(size=2, show.legend=T) + 
  geom_smooth(method=lm, se=F, show.legend=T) +
  geom_abline(data=sim1.plot2, aes(intercept=int2, slope=slp2, 
    linetype=model), size=1, show.legend=T) +
  theme(legend.title = element_blank()) +
  scale_x_continuous(name="V1",
                     limits=c(0,20)) +
  scale_y_continuous(name="V2", limits=c(0,40))+
  guides(colour = "none")


```

## Another thing is different: CENTERING {.build}

2 Options:

* **grand mean centering** 
  - centering like in normal regression
  - linear transformation -> only intercept changes (model is equivalent)
* **group mean centering**
  - subtract the individual's group mean from the individual's score
  - parameters change -> model is NOT equivalent
  - group means can be added as group predictors (to disentangle micro and macro level
contributions)


## When to center how? {.build .smaller .reduceTopMarginText}

This is a complex question (e.g., see [Hofman & Gavin (1998)](https://doi.org/10.1016/S0149-2063(99)80077-4), [Paccagnella (2006)](http://journals.sagepub.com/doi/10.1177/0193841X05275649), [Enders & Tofighi (2007)](http://dx.doi.org/10.1037/1082-989X.12.2.121), and [Hamaker & Grasman (2015)](https://www.frontiersin.org/articles/10.3389/fpsyg.2014.01492) for discussions) with no easy answer.

* Raw:
  - if one is interested in intercept and intercept variance when predictor is 0
  
* Grand mean centering:
  - often used for higher level variable to facilitate interpretation
  - interest in a L2 predictor and want to control for L1 covariates
  - interest in interactions between L2 variables
  
* Group mean centering:
  - purpose is disentangling effects on different levels (add group means as predictor)
  - if multilevel collinearity is high (e.g., student's age at L1 and school level at L2)
  - L1 association between $X$ and $Y$ is of substantive interest
  - cross-level interactions and L1 interactions
  
**Correct centering depends solely on your question!** Use different centerings for different questions (even in 1 analysis block).


## Let's look at a practical example {.build .smaller}

We will use data from the article [Determinants of healthcare worker turnover in intensive care units: A micro-macro multilevel analysis](https://doi.org/10.1371/journal.pone.0251779) by Daouda, Hocine & Temime (2021).

Variables:

* `stress level` ... DV (nurse's reported stress level)
* `support from colleagues` ... L1 IV (per nurse)
* `type of ICU` ... L2 IV (per unit)

To disentangle L1 and L2 effect of `support from colleagues` we will _group mean center_ it and also use its _group means_ as a predictor.

So, instead of `support from colleagues` we will use:

* `support_l1`
* `support_l2`

Further, `support_l2` will be _grand mean centered_.

```{r group_mean_centering, include=FALSE}
df_ICU <- df_ICU %>%
  group_by(CodeService) %>% # CodeService = unit id
  mutate(support_l2 = mean(`support from colleagues`, na.rm=TRUE)) %>%
  ungroup() %>%
  mutate(
    # create support_l1
    support_l1 = `support from colleagues` - support_l2,
    # grand mean center rest of the variables
    across(all_of(c('support_l2')), scale, scale=FALSE)
  )
```


## Inital exploratory analyses {.build}

Things you know from previous sessions:

* skim through the data
* univariate summaries (numerical + graphical)
* bivariate summaries (numerical + graphical)
* bivariate summaries faceted/grouped by higher level units

## Example of bivariate summaries faceted/grouped by higher level units

```{r, echo=FALSE, message=FALSE}
df_ICU %>% 
  ggplot(aes(y=`stress level`, x=support_l1)) +
  geom_point(aes(alpha=0.3, size=support_l2, color=`type of ICU`)) +
  geom_smooth(method = "lm", color='red') +
  facet_wrap(vars(CodeService))

```

## Model Building Strategy {.build .smaller}

* start simple (establish a baseline for evaluating larger models)
* add covariates one at a time, level by level, maybe centering certain variables
* finally, examine the random effects and variance components, beginning with a full set of error terms and then removing covariance terms and variance terms where advisable (for instance, when parameter estimates are failing to converge or producing impossible or unlikely values) 

This strategy follows closely [Raudenbush & Bryk (2002)](https://us.sagepub.com/en-us/nam/hierarchical-linear-models/book9230) approach.

Other strategies are possible. E.g., [Diggle et al. (2002)](https://global.oup.com/academic/product/analysis-of-longitudinal-data-9780199676750?cc=at&lang=en&) begins with a saturated fixed effects model, determines variance components based on that, and then simplifies the fixed part of the model after fixing the random part.




## Digression: Random vs. Fixed Effects {.build .smaller}

**fixed effects**

* levels of a factor we want to draw inferences from
* would not change in replications of the study
* e.g., `type of ICU`

**random effects**

* levels of a factor which is just a sample from a larger population of factor levels
* not interested in drawing conclusions about specific levels
* but, interested in accounting for the influence of the random effect
* e.g., specific unit



## An Initial Model: Random Intercepts {.build .smaller .columns-2}

* no predictors at either level
* assess variation at each level

```{r model_0 Random Intercepts, class.source='bottomMargin-10'}
model_0 <- lmer(`stress level` ~ 
  1+(1|CodeService), REML=T, data=df_ICU)
```
```{r model_0 print, echo=FALSE, message=FALSE, class.source='bottomMargin-10'}
summary(model_0)
```

* $\hat{\alpha}_0=22.66$ = est. mean stress level across all units
* $\hat{\sigma}²=21.73$ = est. variance in within-units deviations
* $\hat{\sigma}_{u}^{2}=0.79$ = est. variance in between-units deviations

**intraclass correlation coefficient (ICC)**

$\hat{\rho}=\frac{\hat{\sigma}_{u}^{2}}{\hat{\sigma}_{u}^{2}+\hat{\sigma}²}$ = `r round(0.7933/(0.7933+22.66),3)`

`r round(0.7933/(0.7933+22.66),3)*100`% of the total stress level variability are attributable to differences among units.

As $\rho$ approaches 0, nurses stress levels are essentially independent. The **effective sample size** is close to number of nurses. As $\rho$ approaches 1, all stress levels of all nurses in one unit become equal. The **effective sample size** is close to number of units.


## Digression: Model estimation {.build }

In multilevel models, parameters are estimated with likelihood-based methods (instead of OLS). The most common methods are:

* maximum likelihood (ML)
* restricted maximum likelihood (REML)

REML accounts for loss in degrees of freedom from fixed effects estimation, and provides an unbiased estimate of variance components. Therefore, it is preferable in most cases.

ML should be used if nested **fixed** effects models are being compared using a likelihood ratio test (nested random are fine with REML).

If you want to know more about this topic, then take a look at [Singer & Willet (2003)](https://academic.oup.com/book/41753).


## Random Slopes and Intercepts Model {.build .smaller .reduceTopMarginCode .columns-2}

```{r model_1, class.source='bottomMargin-10'}
model_1<-lmer(`stress level` ~ support_l1+
  (support_l1|CodeService), REML=T, data=df_ICU)
```
```{r model_1 print, echo=FALSE, message=FALSE, class.source='bottomMargin-10'}
summary(model_1)
```

- $\hat{\alpha}_{0}=22.6$ ... mean stress level at mean `support_l1` 
- $\hat{\beta}_{0}=-0.007$ ... mean decrease of stress level if `support_l1` increases by 1
- $\hat{\sigma}^2=21.2$ ... variance in within-unit deviations
- $\hat{\sigma}_{u}^{2}=0.85$ ... variance in between-units deviations
- $\hat{\sigma}_{v}^{2}=0.19$ ... variance in between-`support_l1` deviations
- $\hat{\rho}_{uv}=-0.45$ ... correlation of unit intercept and `support_l1`

## Visualization of model_1

```{r, echo=FALSE, message=FALSE}
ints_1 = fixef(model_1)[1] + ranef(model_1)[[1]][1]
slopes_1 = fixef(model_1)[2] + ranef(model_1)[[1]][2]
model_1.plot = data.frame(unit = levels(df_ICU$CodeService),
                         ints_1 = ints_1[[1]],
                         slopes_1 = slopes_1[[1]])

p <- ggplot() +  
  geom_abline(data = model_1.plot, 
              aes(intercept = ints_1, slope = slopes_1, color = unit)) +
  geom_abline(aes(intercept = fixef(model_1)[1],
                  slope = fixef(model_1)[2]), size = 1) +
  scale_x_continuous(name = "support_l1", limits = c(-6,6)) +
  scale_y_continuous(name="stress level", limits = c(15,30))  
  
ggplotly(p) #%>%
  #config(displayModeBar = FALSE)
```


## Digression: Parameter tests {.build .smaller}

Getting p-values in multilevel models is not trivial, because the exact distribution of the test statistics under the null hypothesis (no fixed effect) is unknown [(Bates et al. 2015)](https://www.jstatsoft.org/v067/i01). 

Approaches:

* t-values (ratios of parameter estimates to estimated standard errors) with absolute value above 2 indicate significant evidence that a particular model parameter is different than 0
* tests based on conservative assumptions, large-sample results, or approximate degrees of freedom for a t-distribution
* bootstrap, e.g.,
  + parametric [(Efron, 2012)](https://doi.org/10.1214/12-AOAS571)
  + residual [(Carpenter, Goldstein and Rasbash, 2003)](https://doi.org/10.1111/1467-9876.00415)
  + wild [(Modugno and Giannerini, 2015)](https://doi.org/10.1080/03610926.2013.802807)

## Example: Parametric Bootstrap

The parametric bootstrap simulates bootstrap samples from the estimated distribution functions. That is, error terms and random effects are simulated from their estimated normal distributions and are combined into bootstrap samples via the fitted model equation.

```{r, cache=TRUE}
boot_model_1 <- lmeresampler::bootstrap(
  model_1, .f = fixef, type = "parametric", B = 1000)
confint(boot_model_1, type = "perc")
```

## Add level 2 predictor {.build .smaller .columns-2}

```{r model_2, class.source='bottomMargin-5'}
model_2<-lmer(`stress level`~support_l1+support_l2+
  (support_l1|CodeService), REML=T, data=df_ICU)
```
```{r model_2 print, echo=FALSE, message=FALSE, class.source='bottomMargin-10'}
summary(model_2)
```


Like `support_l1`, increase of `support_l2` slightly reduces the stress level (-0.15), however, both effects are not significant (not tested, but | _t_ | is far away from 2)


## Add second level 2 predictor {.build .smaller}

```{r model_3, class.source='bottomMargin-5'}
model_3 <- lmer(`stress level` ~ `type of ICU` + support_l1 + support_l2 +
  (support_l1 | CodeService), REML=T, data=df_ICU)

model_3_ML <- lmer(`stress level` ~ `type of ICU` + support_l1 + support_l2 +
  (support_l1 | CodeService), REML=F, data=df_ICU)
```
```{r model_3 print, echo=FALSE, message=FALSE, class.source='bottomMargin-10'}
VCrandom <- VarCorr(model_3)
print(VCrandom, comp = c("Variance", "Std.Dev."))
cat(" Number of Level Two groups = ",
    summary(model_3)$ngrps)
coef(summary(model_3))
```

Neither, _ICU Type 1_ or _2_ seem to differ significantly from _Type 0_. But it seems like _1_ and _2_ could differ.

How would we test that?


## Try to simplify model_3 by removing random slope {.smaller}

```{r model_4, class.source='bottomMargin-5'}
model_4 <- lmer(`stress level` ~ `type of ICU` + support_l1 + support_l2 +
  (1 | CodeService), REML=T, data=df_ICU)

model_4_ML <- lmer(`stress level` ~ `type of ICU` + support_l1 + support_l2 +
  (1 | CodeService), REML=F, data=df_ICU)
```
```{r model_4 print, echo=FALSE, message=FALSE, class.source='bottomMargin-10'}
VCrandom <- VarCorr(model_4)
print(VCrandom, comp = c("Variance", "Std.Dev."))
cat(" Number of Level Two groups = ",
    summary(model_4)$ngrps)
coef(summary(model_4))
```

## Compare model_3 and model_4 (1/2){.build}

Information criteria like AIC and BIC can be used to compare nested and not nested models (smaller is better).

Likelihood ratio tests can be used for nested models.

**Are model_3 and model_4 nested?**

```{r, class.source='bottomMargin-5'}
anova(model_4, model_3, test='Chisq')
```

## Compare model_3 and model_4 (2/2){.build .smaller}

Let's try bootstrapped likelihood ratio test (BLRT).

```{r, class.source='bottomMargin-5', cache=TRUE}
b3 <- lmeresampler::bootstrap(model_3_ML, .f = logLik, type = "parametric", B = 1000)
b4 <- lmeresampler::bootstrap(model_4_ML, .f = logLik, type = "parametric", B = 1000)
lrt_b <- -2 * b4$replicates + 2 * b3$replicates
quantile(lrt_b, probs = c(.025, .975))
```
CI does include 0 -> no model is significantly better.

-> ICs indicate better fit of `model_4` and, although more parsimonious, it doesn't fit the data significantly worse than `model_3`.

If your theory doesn't emphasize the random slope, than just the random intercept seems to be sufficient.

**Always pick the model that fits your theory**. 

_Note_: Theory is fixed before the analysis.


## Homework (graded) | due till 2022-12-08   {.flexbox .vcenter}

Use the data from the article [Determinants of healthcare worker turnover in intensive care units: A micro-macro multilevel analysis](https://doi.org/10.1371/journal.pone.0251779) by Daouda, Hocine & Temime (2021).

**Test the hypotheses:**

The stress level is affected by:

**H1**: L1 variable `support from supervisors`

**H2**: L2 variable `staff-to-patient ratio day`

Tip: you can reuse code from code that generated today's slides



## Thank you for your attention! {.flexbox .vcenter}

Next Time (2022-12-15):

**Missing Data**

_Note:_ There will be just one "your pick"-session, because homework was more work than I have expected at the start of the semester.
