---
title: "Mean differences - Part I"
author: "Bernhard Piskernik"
date: "2022/10/06"
output: 
  ioslides_presentation:
        css: ./style.css
        incremental: true
    
---

```{r setup, include=FALSE}
library(tidyverse)
library(plotly)
library(kableExtra)
options(warn=-1)
```

## The Basic Problem{.build}

Hypotheses about mean differences are very common. 

Typical question are:

Does the mean 

* differ between group A and group B? **between design**
* of group A change from time 1 to time 2 (e.g., before and after treatment)? **within design**
* change from time 1 to time 2 between group A and group B differ? **mixed design**


## Between vs Within Design

|                                         | BETWEEN | WITHIN | example/explanation                                                       |
|-----------------------------------------|---------|--------|---------------------------------------------------------------------------|
| learning and transfer across conditions | no      | yes    | comparing difficulty of IQ-tests                                          |
| effort per participant                  | low     | high   | just one, instead of multiple tests                                       |
| difficulty to set up                    | low     | high   | comparing IQ-tests: random assignment VS. random sequence per participant |
| random noise                            | high    | low    | potential confounding factors are hold constant by using the same person  |
| required sample size                    | high    | low    | participants contribute multiple data points + less noise                 |

## Sample size example (2 groups) {.build}

```{r}
params <- list(sig.level = 0.05, power = 0.80, d = 0.25)
```


```{r}
do.call(pwr::pwr.t.test, c(type='two.sample', params)) %>% .$n
```

*between*: `r do.call(pwr::pwr.t.test, c(type='two.sample', params)) %>% .$n %>% ceiling()` participants per group


```{r}
do.call(pwr::pwr.t.test, c(type='paired', params)) %>% .$n
```
*within*: `r do.call(pwr::pwr.t.test, c(type='paired', params)) %>% .$n %>% ceiling()` pairs


## When is comparing the mean appropriate? | Appropriateness of mean as measure of central tendency

* the measure is at least interval scaled (there is order and differences are meaningful)
* unimodal distribution in each group
* symmetric distribution in each group (not too much skew)
* no outliers

## @unimodal

```{r, fig.height=4}
bimodal <- tibble(data=FamilyRank::rbinorm(1000, -1, 1, .4, .4, .5))
ggplot(bimodal, aes(data)) +
  geom_histogram(bins = 40) +
  geom_vline(aes(xintercept = mean(bimodal$data)),col='red',size=2)
```

## @symmetric

```{r, fig.height=4}
gamma <- tibble(data=rgamma(1000, 2, 1))
ggplot(gamma, aes(data)) +
  geom_histogram(bins = 40) +
  geom_vline(aes(xintercept = mean(gamma$data)),col='red',size=2)

```

## @outlier

```{r, fig.height=4}
out <- tibble(data=c(rnorm(980), rnorm(20, 50, 5)))
ggplot(out, aes(data)) +
  geom_histogram(bins = 100) +
  geom_vline(aes(xintercept = mean(out$data)),col='red',size=2)
```

## How to test for differences in means?

* most typical: **An**alysis **o**f **Va**riance
* main benefit - it's versatile:
  * multiple groups
  * multiple factors
  * between, within, and mixed design variants
  * multiple measures
  * allows covariates
* main drawback - if assumptions are met it is BLUE, if not it becomes liberal (*p* of rejecting H0 is too big)
  * normal distribution
  * variance homogeneity
  
## The issue of testing the assumptions

* pre-test are statistical tests as well. They have their own Type I and Type II errors and are dependent on *n*.
* this causes a special form of a multiple testing scenario
* sequential: typical corrections like Bonferroni or [Benjamini-Hochberg](https://www.jstor.org/stable/2346101) for parallel testing don't apply
* consequence: unknown final type-I- and type-II-risks (see, e.g., [Rasch et al., 2011](https://link.springer.com/article/10.1007/s00362-009-0224-x))
* Recommendation: **if there is a good, robust alternative, then use it right from the start**
* at least robust against violation of variance homogeneity, because this is the bigger issue than deviation from normal distribution if the mean is a appropriate measure of the central tendency (see [before](#4))


## Robust Alternatives {.smaller}

* parametric corrections: 
  - between: [Welch's ANOVA](https://doi.org/10.2307/2332579)
  - within: [Greenhouse-Geisser](https://doi.org/10.1007/BF02289823)
  
  
* non-parametric:
  - distributional tests (e.g. [Kruskal-Wallis Test](https://doi.org/10.2307/2280779))
    + low power
    + only applicable if distributions are the same except for location shift
    + not available for many designs
  - permutation tests
    + exact test
    + can become computationally expensive
    
* make parametric more robust with
  - trimmed means
  - bootstrapping
  - transformations


## Digression: Resampling Methods | Permutation test {.build .smaller}

Base assumption: the sample groups are drawn from the same distribution (**H0**) --> switching participants between groups shouldn't matter.

Steps:

1. value of interest (e.g., difference of mean between groups) is measured in sample.
2. all data is pooled
3. all permutations, that result in the original group sizes are drawn (then it is exact, often just a sample is used)
4. value of interest is calculated in each permutation
5. the original value is compared to all others
6. if it is bigger than 95%, then the H0 is rejected (α=.05, one-sided)
7. for a two-sided test the absolute value is compared with all absolute values

For a nice visual explanation see [here](https://www.jwilber.me/permutationtest/).


## Digression: Resampling Methods | Bootstrapping {.build .smaller}

Base concept: **random sampling with replacement**

Allows the estimation of the *sampling distribution* of almost any statistic, which can be used for measures of accuracy (e.g., confidence intervals)


```{r,  echo=FALSE, message=FALSE, fig.height=4}
# code not shown in presentation
set.seed(42)

gamma <- rgamma(100, 0.5, 1)

m <- mean(gamma)
# +/- 2.58 * standard error of mean to get CIs
lower <- m - 2.58*sd(gamma)/sqrt(length(gamma))
upper <- m + 2.58*sd(gamma)/sqrt(length(gamma))


p1 <- plot_ly(x = ~gamma, type = 'histogram', nbinsx = 20) %>%
  add_segments(x=m, y=0, xend=m, yend=35, line=list(color="orange", width = 4)) %>%
  add_segments(x=lower, y=0, xend=lower, yend=35, line=list(color="orange", width = 4, dash = "dash")) %>%
  add_segments(x=upper, y=0, xend=upper, yend=35, line=list(color="orange", width = 4, dash = "dash")) %>% 
  layout(showlegend = FALSE)

# define function that calculates stat of interest 
getMean <- function(vector, i){
  return(mean(vector[i]))
}

boot_mean <- boot::boot(gamma, getMean, R=10000) 

means = c(boot_mean$t)
means_m <- mean(means)
# get use the percentiles of the sampled means for the CIs
means_lower <- quantile(means, .005)
means_upper <- quantile(means, .995)

p2 <- plot_ly(x = ~means, type = 'histogram', nbinsx = 50, color = 'orange') %>%
  add_segments(x=means_m, y=0, xend=means_m, yend=1275, line=list(color="green", width = 4)) %>%
  add_segments(x=means_lower, y=0, xend=means_lower, yend=1275, line=list(color="green", width = 4, dash = "dash")) %>%
  add_segments(x=means_upper, y=0, xend=means_upper, yend=1275, line=list(color="green", width = 4, dash = "dash"))  %>%
  add_segments(x=m, y=0, xend=m, yend=1275, line=list(color="blue", width = 4)) %>%
  add_segments(x=lower, y=0, xend=lower, yend=1275, line=list(color="blue", width = 4, dash = "dash")) %>%
  add_segments(x=upper, y=0, xend=upper, yend=1275, line=list(color="blue", width = 4, dash = "dash")) %>% 
  layout(showlegend = FALSE) %>% 
  layout(title="Left: orginal data<br>Right: Resampled Means (R=10000)<br>Mean + 99% CI (green = bootstrap, blue = original)") %>% 
  config(displayModeBar = F)

subplot(p1, p2)
```
<div class="notes">
Why is Bootstrap CI moved to the right compared with the symmetric CI based on the SE? 
</div>

## Digression: Trimmed Means {.smaller .build}

Base concept: use only the middle 95/90/80 % of the samples

* applied to achieve fewer sampling fluctuations
* rationale: extreme values are unrepresentative but strongly impact the mean


```{r,  echo=FALSE, message=FALSE, fig.height=4.5}
# code not shown in presentation
set.seed(42)

gamma <- rgamma(100, 0.5, 1)

m <- mean(gamma)
m5 <- mean(gamma, trim=.05)
m10 <- mean(gamma, trim=.10)
m20 <- mean(gamma, trim=.20)



p1 <- plot_ly(x = ~gamma, type = 'histogram', nbinsx = 20, name='data') %>%
  add_segments(x=m, y=0, xend=m, yend=35, line=list(color="orange", width = 4), name='mean') %>%
  add_segments(x=m5, y=0, xend=m5, yend=35, line=list(color="yellow", width = 4), name = "5% trimmed") %>%
  add_segments(x=m10, y=0, xend=m10, yend=35, line=list(color="palegreen", width = 4), name = "10% trimmed") %>% 
  add_segments(x=m20, y=0, xend=m20, yend=35, line=list(color="turquoise", width = 4), name = "20% trimmed") %>% 
  config(displayModeBar = F) 

p1
```


## Digression: Transformation (Part 1/2){.build .smaller}

Used to convert non-normally distributed data into normally distributed data.

* most frequent transformation: **log**, but not necessarily the best choice
* if there is no theoretical reason for a specific transformation, use [Box-Cox Power Transformation](https://www.jstor.org/stable/2984418) to find the "best fitting" one 
* Box-Cox transform is used to find the optimal $\lambda$ in $\displaystyle \frac{y^{\lambda} - 1}{\lambda}$

```{r,  echo=FALSE, message=FALSE, fig.height=3.5}
set.seed(42)

raw <- rgamma(1000, 0.5, 1)
bc <- MASS::boxcox(raw~1, plotit=FALSE)
lambda <- bc$x[which.max(bc$y)]
transformed <- (raw ^ lambda - 1)/lambda 

df <- tibble(raw = raw, transformed = transformed) %>%
  pivot_longer(cols = c('raw', 'transformed'), names_to = 'type', values_to = 'values')

p <- ggplot(df, aes(x=values, fill=type)) +
  geom_histogram(position = "identity", alpha = 0.8, bins = 50)

ggplotly(p) %>%
  config(displayModeBar = FALSE)


```


## Digression: Transformation (Part 2/2) | Why you might not want to use it! {.build}

* Box-Cox works only for positive values (solution: shift into positive range or [Yeo–Johnson transformation](https://doi.org/10.1093/biomet/87.4.954))
* Interpretation is hard (hardly anybody understands logs, much less arbitrary power transformations)
* it is not a linear transformation, therefore the back transformation can [introduce bias](https://www.tandfonline.com/doi/pdf/10.1080/00063658809476992)
* e.g., the raw mean in the previous example was `r mean(raw) %>% round(3)`, but back-transforming the mean of the transformed values yields `r (mean(transformed)*lambda+1)^(1/lambda) %>% round(3)`



## Digression: sum of squares types | Type I {.build}

**Type I Sums of Squares** is sequential 

1. assign a maximum of variation to first variable
2. in the remaining variation, assign the maximum of variation to next variable
3. repeat 2 for all remaining variables
4. in the remaining variation, assign the maximum of variation to first interaction effect
5. in the remaining variation, assign the maximum of variation to next interaction effect
6. repeat 5 for all remaining interaction effects
7. and assign the rest to the Residual Sums of Squares

Use _Type I_ if you have theoretical considerations to inflate the importance of one of your variables


## Digression: sum of squares types | Type II & III {.build}

**Type II Sums of Squares** doesn't "see" interactions

* sequence doesn't matter
* the variation assigned to each independent variable is accounting for all other independent variables
* it assumes no interaction effect (if this is true, then it is more powerful than _Type III_)

**Type III Sums of Squares** aka partial sums of squares 

* like II, but with interaction effects

Except for true experiments, Type III is preferable because testing for an interaction effect is yet another condition we would need to test first.


## Homework | **NOT** graded  {.flexbox .vcenter}

* create an account at [SwissUbase](https://www.swissubase.ch/)
* download the [MOSAiCH](https://doi.org/10.48573/t659-e039) data set
* create an [OSF](https://osf.io/) account
* download [this](https://doi.org/10.17605/OSF.IO/ZNCWA) data set


## Thank you for your attention! {.flexbox .vcenter}


Next Time:

**Mean differences - Practical Examples**

